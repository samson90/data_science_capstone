blogs_content <- readLines(blogs_file, encoding="UTF-8")
close(blogs_file)
news_file <- file("final/en_US/en_US.news.txt", open="rb")
news_content <- readLines(news_file, encoding="UTF-8")
close(news_file)
# print summary statistics.
library(data.table)
twitter_word_count = sapply(gregexpr("\\W+", twitter_content), length) + 1
blogs_word_count = sapply(gregexpr("\\W+", blogs_content), length) + 1
news_word_count = sapply(gregexpr("\\W+", news_content), length) + 1
content_summary = data.table(FILE=c("en_US.twitter.txt", "en_US.blogs.txt", "en_US.news.txt"),
LINECOUNT=c(length(twitter_content), length(blogs_content), length(news_content)),
WORDCOUNT=c(sum(twitter_word_count), sum(blogs_word_count), sum(news_word_count)))
print(content_summary)
content = c(twitter_content, blogs_content, news_content)
set.seed(13690)
twitter_content = twitter_content[sample(1:length(twitter_content), length(twitter_content)*.01)]
blogs_content = blogs_content[sample(1:length(blogs_content), length(blogs_content)*.01)]
news_content = news_content[sample(1:length(news_content), length(news_content)*.01)]
content = c(twitter_content, blogs_content, news_content)
remove(twitter_content, blogs_content, news_content)
# remove any character that is not an alpha-numeric or space
content <- gsub("[^a-z0-9 ]", "", content)
# convert to all lowercase
content = tolower(content)
# remove any extra whitespace and trailing or ending whitespace.
content <- gsub(" +( )", "\\1", content)
content <- gsub("^ *| *$", "", content)
#read in list of curse words.
curse_words = readLines("final/bad_words.txt")
# remove the formatting
curse_words = gsub("\\\"|:1,|:1", "", curse_words)
# remove trailing whitespace
curse_words = gsub("\\s$", "", curse_words)
# add backslashes to some of the punctuation so we can use the words in a regular
# expression
curse_words = gsub("(\\!|\\+|\\.|-|\\*)", "\\\\\\1", curse_words)
# add the word boundaries
curse_words = paste(curse_words, collapse = "\\b|\\b")
curse_words = paste(paste("\\b", curse_words, sep=""), "\\b", sep="")
# remove the curse words from the corpus and replace them with '*explicit'
content = gsub(curse_words, "*explicit", content)
# create 1-gram, 2-gram, 3-gram, and 4-gram term-document matrices.
library(RWeka)
library(tm)
us_Corpus <- VCorpus(VectorSource(content))
options(mc.cores=1)
one_gram_tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
one_tdm <- TermDocumentMatrix(us_Corpus, control=list(tokenizer=one_gram_tokenizer))
two_gram_tokenizer = function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
two_tdm <- TermDocumentMatrix(us_Corpus, control=list(tokenizer=two_gram_tokenizer))
three_gram_tokenizer = function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
three_tdm = TermDocumentMatrix(us_Corpus, control=list(tokenizer=three_gram_tokenizer))
four_gram_tokenizer = function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
four_tdm = TermDocumentMatrix(us_Corpus, control=list(tokenizer=four_gram_tokenizer))
library(ggplot2)
library(Matrix)
library(wordcloud)
library(RColorBrewer)
# Create a wordcloud of most common n-grams
plot_wordcloud = function(tdm, title){
m <- sparseMatrix(i=tdm$i, j=tdm$j, x=tdm$v, dims=c(tdm$nrow, tdm$ncol))
sorted_grams <- sort(rowSums(m), decreasing=TRUE, index.return = TRUE)
common_grams <- head(sorted_grams$x,50)
common_ix <- head(sorted_grams$ix,50)
terms <- factor(tdm$dimnames$Terms[common_ix], levels=tdm$dimnames$Terms[common_ix]);
wordcloud(terms, common_grams, colors=brewer.pal(8, "Dark2"))
sorted_grams
}
sorted_one_grams = plot_wordcloud(one_tdm, "Most Common 1-Gram Terms")
#sorted_two_grams = plot_common_terms(two_tdm, "Most Common 2-Gram Terms")
#sorted_three_grams = plot_common_terms(three_tdm, "Most Common 3-Gram Terms")
#sorted_four_grams = plot_common_terms(four_tdm, "Most Common 4-Gram Terms")
library(ggplot2)
library(Matrix)
library(wordcloud)
library(RColorBrewer)
# Create a wordcloud of most common n-grams
plot_wordcloud = function(tdm, title){
m <- sparseMatrix(i=tdm$i, j=tdm$j, x=tdm$v, dims=c(tdm$nrow, tdm$ncol))
sorted_grams <- sort(rowSums(m), decreasing=TRUE, index.return = TRUE)
common_grams <- head(sorted_grams$x,50)
common_ix <- head(sorted_grams$ix,50)
terms <- factor(tdm$dimnames$Terms[common_ix], levels=tdm$dimnames$Terms[common_ix]);
wordcloud(terms, common_grams, colors=brewer.pal(7, "Dark2"))
sorted_grams
}
sorted_one_grams = plot_wordcloud(one_tdm, "Most Common 1-Gram Terms")
#sorted_two_grams = plot_common_terms(two_tdm, "Most Common 2-Gram Terms")
#sorted_three_grams = plot_common_terms(three_tdm, "Most Common 3-Gram Terms")
#sorted_four_grams = plot_common_terms(four_tdm, "Most Common 4-Gram Terms")
library(ggplot2)
library(Matrix)
library(wordcloud)
library(RColorBrewer)
# Create a wordcloud of most common n-grams
plot_wordcloud = function(tdm, title){
m <- sparseMatrix(i=tdm$i, j=tdm$j, x=tdm$v, dims=c(tdm$nrow, tdm$ncol))
sorted_grams <- sort(rowSums(m), decreasing=TRUE, index.return = TRUE)
common_grams <- head(sorted_grams$x,50)
common_ix <- head(sorted_grams$ix,50)
terms <- factor(tdm$dimnames$Terms[common_ix], levels=tdm$dimnames$Terms[common_ix]);
wordcloud(terms, common_grams, colors=brewer.pal(6, "Dark2"))
sorted_grams
}
sorted_one_grams = plot_wordcloud(one_tdm, "Most Common 1-Gram Terms")
#sorted_two_grams = plot_common_terms(two_tdm, "Most Common 2-Gram Terms")
#sorted_three_grams = plot_common_terms(three_tdm, "Most Common 3-Gram Terms")
#sorted_four_grams = plot_common_terms(four_tdm, "Most Common 4-Gram Terms")
library(ggplot2)
library(Matrix)
library(wordcloud)
library(RColorBrewer)
# Create a wordcloud of most common n-grams
plot_wordcloud = function(tdm, title){
m <- sparseMatrix(i=tdm$i, j=tdm$j, x=tdm$v, dims=c(tdm$nrow, tdm$ncol))
sorted_grams <- sort(rowSums(m), decreasing=TRUE, index.return = TRUE)
common_grams <- head(sorted_grams$x,50)
common_ix <- head(sorted_grams$ix,50)
terms <- factor(tdm$dimnames$Terms[common_ix], levels=tdm$dimnames$Terms[common_ix]);
wordcloud(terms, common_grams, colors=brewer.pal(6, "Dark2"))
sorted_grams
}
#sorted_one_grams = plot_wordcloud(one_tdm, "Most Common 1-Gram Terms")
sorted_two_grams = plot_common_terms(two_tdm, "Most Common 2-Gram Terms")
#sorted_three_grams = plot_common_terms(three_tdm, "Most Common 3-Gram Terms")
#sorted_four_grams = plot_common_terms(four_tdm, "Most Common 4-Gram Terms")
library(ggplot2)
library(Matrix)
library(wordcloud)
library(RColorBrewer)
# Create a wordcloud of most common n-grams
plot_wordcloud = function(tdm, title){
m <- sparseMatrix(i=tdm$i, j=tdm$j, x=tdm$v, dims=c(tdm$nrow, tdm$ncol))
sorted_grams <- sort(rowSums(m), decreasing=TRUE, index.return = TRUE)
common_grams <- head(sorted_grams$x,50)
common_ix <- head(sorted_grams$ix,50)
terms <- factor(tdm$dimnames$Terms[common_ix], levels=tdm$dimnames$Terms[common_ix]);
wordcloud(terms, common_grams, colors=brewer.pal(6, "Dark2"))
sorted_grams
}
#sorted_one_grams = plot_wordcloud(one_tdm, "Most Common 1-Gram Terms")
sorted_two_grams = plot_wordcloud(two_tdm, "Most Common 2-Gram Terms")
#sorted_three_grams = plot_wordcloud(three_tdm, "Most Common 3-Gram Terms")
#sorted_four_grams = plot_wordcloud(four_tdm, "Most Common 4-Gram Terms")
library(ggplot2)
library(Matrix)
library(wordcloud)
library(RColorBrewer)
# Create a wordcloud of most common n-grams
plot_wordcloud = function(tdm, title){
m <- sparseMatrix(i=tdm$i, j=tdm$j, x=tdm$v, dims=c(tdm$nrow, tdm$ncol))
sorted_grams <- sort(rowSums(m), decreasing=TRUE, index.return = TRUE)
common_grams <- head(sorted_grams$x,50)
common_ix <- head(sorted_grams$ix,50)
terms <- factor(tdm$dimnames$Terms[common_ix], levels=tdm$dimnames$Terms[common_ix]);
wordcloud(terms, common_grams, colors=brewer.pal(6, "Dark2"))
sorted_grams
}
#sorted_one_grams = plot_wordcloud(one_tdm, "Most Common 1-Gram Terms")
#sorted_two_grams = plot_wordcloud(two_tdm, "Most Common 2-Gram Terms")
sorted_three_grams = plot_wordcloud(three_tdm, "Most Common 3-Gram Terms")
#sorted_four_grams = plot_wordcloud(four_tdm, "Most Common 4-Gram Terms")
library(ggplot2)
library(Matrix)
library(wordcloud)
library(RColorBrewer)
# Create a wordcloud of most common n-grams
plot_wordcloud = function(tdm, title){
m <- sparseMatrix(i=tdm$i, j=tdm$j, x=tdm$v, dims=c(tdm$nrow, tdm$ncol))
sorted_grams <- sort(rowSums(m), decreasing=TRUE, index.return = TRUE)
common_grams <- head(sorted_grams$x,50)
common_ix <- head(sorted_grams$ix,50)
terms <- factor(tdm$dimnames$Terms[common_ix], levels=tdm$dimnames$Terms[common_ix]);
wordcloud(terms, common_grams, colors=brewer.pal(6, "Dark2"))
sorted_grams
}
#sorted_one_grams = plot_wordcloud(one_tdm, "Most Common 1-Gram Terms")
#sorted_two_grams = plot_wordcloud(two_tdm, "Most Common 2-Gram Terms")
#sorted_three_grams = plot_wordcloud(three_tdm, "Most Common 3-Gram Terms")
sorted_four_grams = plot_wordcloud(four_tdm, "Most Common 4-Gram Terms")
sorted_four_grams
head(sorted_four_grams)
head(sorted_four_grams, 10)
?head
head(sorted_four_grams$x)
head(sorted_four_grams$ux)
head(sorted_four_grams$ix)
head(four_tdm$dimnames$Terms[sorted_four_grams$ix])
head(four_tdm$dimnames$Terms[sorted_four_grams$ix], 30)
gsub(" +( )", "\\1", "at the university of")
content <- gsub("^ *| *$", "", "at the university of")
gsub("^ *| *$", "", "at the university of")
gsub("[^a-z0-9 ]", "", "at the university of")
# read in three files.
twitter_file <- file("final/en_US/en_US.twitter.txt", open="rb")
twitter_content <- readLines(twitter_file, encoding="UTF-8", skipNul=TRUE)
close(twitter_file)
blogs_file <- file("final/en_US/en_US.blogs.txt", open="rb")
blogs_content <- readLines(blogs_file, encoding="UTF-8")
close(blogs_file)
news_file <- file("final/en_US/en_US.news.txt", open="rb")
news_content <- readLines(news_file, encoding="UTF-8")
close(news_file)
# print summary statistics.
library(data.table)
twitter_word_count = sapply(gregexpr("\\W+", twitter_content), length) + 1
blogs_word_count = sapply(gregexpr("\\W+", blogs_content), length) + 1
news_word_count = sapply(gregexpr("\\W+", news_content), length) + 1
content_summary = data.table(FILE=c("en_US.twitter.txt", "en_US.blogs.txt", "en_US.news.txt"),
LINECOUNT=c(length(twitter_content), length(blogs_content), length(news_content)),
WORDCOUNT=c(sum(twitter_word_count), sum(blogs_word_count), sum(news_word_count)))
print(content_summary)
set.seed(13690)
twitter_content = twitter_content[sample(1:length(twitter_content), length(twitter_content)*.01)]
blogs_content = blogs_content[sample(1:length(blogs_content), length(blogs_content)*.01)]
news_content = news_content[sample(1:length(news_content), length(news_content)*.01)]
content = c(twitter_content, blogs_content, news_content)
remove(twitter_content, blogs_content, news_content)
# convert to all lowercase
content = tolower(content)
# remove any character that is not an alpha-numeric or space
content <- gsub("[^a-z0-9 ]", "", content)
# remove any extra whitespace and trailing or ending whitespace.
content <- gsub(" +( )", "\\1", content)
content <- gsub("^ *| *$", "", content)
#read in list of curse words.
curse_words = readLines("final/bad_words.txt")
# remove the formatting
curse_words = gsub("\\\"|:1,|:1", "", curse_words)
# remove trailing whitespace
curse_words = gsub("\\s$", "", curse_words)
# add backslashes to some of the punctuation so we can use the words in a regular
# expression
curse_words = gsub("(\\!|\\+|\\.|-|\\*)", "\\\\\\1", curse_words)
# add the word boundaries
curse_words = paste(curse_words, collapse = "\\b|\\b")
curse_words = paste(paste("\\b", curse_words, sep=""), "\\b", sep="")
# remove the curse words from the corpus and replace them with '*explicit'
content = gsub(curse_words, "*explicit", content)
# read in three files.
twitter_file <- file("final/en_US/en_US.twitter.txt", open="rb")
twitter_content <- readLines(twitter_file, encoding="UTF-8", skipNul=TRUE)
close(twitter_file)
blogs_file <- file("final/en_US/en_US.blogs.txt", open="rb")
blogs_content <- readLines(blogs_file, encoding="UTF-8")
close(blogs_file)
news_file <- file("final/en_US/en_US.news.txt", open="rb")
news_content <- readLines(news_file, encoding="UTF-8")
close(news_file)
# print summary statistics.
library(data.table)
twitter_word_count = sapply(gregexpr("\\W+", twitter_content), length) + 1
blogs_word_count = sapply(gregexpr("\\W+", blogs_content), length) + 1
news_word_count = sapply(gregexpr("\\W+", news_content), length) + 1
content_summary = data.table(FILE=c("en_US.twitter.txt", "en_US.blogs.txt", "en_US.news.txt"),
LINECOUNT=c(length(twitter_content), length(blogs_content), length(news_content)),
WORDCOUNT=c(sum(twitter_word_count), sum(blogs_word_count), sum(news_word_count)))
print(content_summary)
set.seed(13690)
twitter_content = twitter_content[sample(1:length(twitter_content), length(twitter_content)*.01)]
blogs_content = blogs_content[sample(1:length(blogs_content), length(blogs_content)*.01)]
news_content = news_content[sample(1:length(news_content), length(news_content)*.01)]
content = c(twitter_content, blogs_content, news_content)
remove(twitter_content, blogs_content, news_content)
# remove any character that is not an alpha-numeric or space
content <- gsub("[^a-zA-Z0-9 ]", "", content)
# convert to all lowercase
content = tolower(content)
# remove any extra whitespace and trailing or ending whitespace.
content <- gsub(" +( )", "\\1", content)
content <- gsub("^ *| *$", "", content)
#read in list of curse words.
curse_words = readLines("final/bad_words.txt")
# remove the formatting
curse_words = gsub("\\\"|:1,|:1", "", curse_words)
# remove trailing whitespace
curse_words = gsub("\\s$", "", curse_words)
# add backslashes to some of the punctuation so we can use the words in a regular
# expression
curse_words = gsub("(\\!|\\+|\\.|-|\\*)", "\\\\\\1", curse_words)
# add the word boundaries
curse_words = paste(curse_words, collapse = "\\b|\\b")
curse_words = paste(paste("\\b", curse_words, sep=""), "\\b", sep="")
# remove the curse words from the corpus and replace them with '*explicit'
content = gsub(curse_words, "*explicit", content)
# create 1-gram, 2-gram, 3-gram, and 4-gram term-document matrices.
library(RWeka)
library(tm)
us_Corpus <- VCorpus(VectorSource(content))
options(mc.cores=1)
one_gram_tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
one_tdm <- TermDocumentMatrix(us_Corpus, control=list(tokenizer=one_gram_tokenizer))
two_gram_tokenizer = function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
two_tdm <- TermDocumentMatrix(us_Corpus, control=list(tokenizer=two_gram_tokenizer))
three_gram_tokenizer = function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
three_tdm = TermDocumentMatrix(us_Corpus, control=list(tokenizer=three_gram_tokenizer))
four_gram_tokenizer = function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
four_tdm = TermDocumentMatrix(us_Corpus, control=list(tokenizer=four_gram_tokenizer))
library(ggplot2)
library(Matrix)
library(wordcloud)
library(RColorBrewer)
# Create a wordcloud of most common n-grams
plot_wordcloud = function(tdm, title){
m <- sparseMatrix(i=tdm$i, j=tdm$j, x=tdm$v, dims=c(tdm$nrow, tdm$ncol))
sorted_grams <- sort(rowSums(m), decreasing=TRUE, index.return = TRUE)
common_grams <- head(sorted_grams$x,50)
common_ix <- head(sorted_grams$ix,50)
terms <- factor(tdm$dimnames$Terms[common_ix], levels=tdm$dimnames$Terms[common_ix]);
wordcloud(terms, common_grams, colors=brewer.pal(6, "Dark2"))
sorted_grams
}
#sorted_one_grams = plot_wordcloud(one_tdm, "Most Common 1-Gram Terms")
#sorted_two_grams = plot_wordcloud(two_tdm, "Most Common 2-Gram Terms")
#sorted_three_grams = plot_wordcloud(three_tdm, "Most Common 3-Gram Terms")
sorted_four_grams = plot_wordcloud(four_tdm, "Most Common 4-Gram Terms")
library(ggplot2)
library(Matrix)
library(wordcloud)
library(RColorBrewer)
# Create a wordcloud of most common n-grams
plot_wordcloud = function(tdm, title){
m <- sparseMatrix(i=tdm$i, j=tdm$j, x=tdm$v, dims=c(tdm$nrow, tdm$ncol))
sorted_grams <- sort(rowSums(m), decreasing=TRUE, index.return = TRUE)
common_grams <- head(sorted_grams$x,50)
common_ix <- head(sorted_grams$ix,50)
terms <- factor(tdm$dimnames$Terms[common_ix], levels=tdm$dimnames$Terms[common_ix]);
wordcloud(terms, common_grams, colors=brewer.pal(6, "Dark2"))
sorted_grams
}
#sorted_one_grams = plot_wordcloud(one_tdm, "Most Common 1-Gram Terms")
#sorted_two_grams = plot_wordcloud(two_tdm, "Most Common 2-Gram Terms")
sorted_three_grams = plot_wordcloud(three_tdm, "Most Common 3-Gram Terms")
#sorted_four_grams = plot_wordcloud(four_tdm, "Most Common 4-Gram Terms")
?wordcloud
library("bitops", lib.loc="~/R/win-library/3.1")
library("RCurl", lib.loc="~/R/win-library/3.1")
library("rmarkdown", lib.loc="~/R/win-library/3.1")
getOption("rpubs.upload.method")
options(rpubs.upload.method="internal")
getOption("rpubs.upload.method")
library("bitops", lib.loc="~/R/win-library/3.1")
library("RCurl", lib.loc="~/R/win-library/3.1")
library("rmarkdown", lib.loc="~/R/win-library/3.1")
getOptions("rpubs.upload.method")
getOption("rpubs.upload.method")
source('~/.active-rstudio-document', echo=TRUE)
grep("a case of", four_tdm)
grep("a case of", four_tdm$dimnames$Terms)
ix <- grep("a case of", four_tdm$dimnames$Terms)
four$tdm$dimnames$Terms
four$tdm$dimnames$Terms[ix]
four_tdm$dimnames$Terms[ix]
four_tdm$dimnames$Terms[grep("would mean the", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("make me the", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("struggling but the", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("date at the", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("be on my", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("in quite some", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("with his little", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("faith during the", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("you must be", four_tdm$dimnames$Terms)]
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
four_tdm$dimnames$Terms[grep("^a case of", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^would mean the", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^make me the", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^struggling but the", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^date at the", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^be on my", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^in quite some", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^with his little", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^faith during the", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^you must be", four_tdm$dimnames$Terms)]
source('~/.active-rstudio-document', echo=TRUE)
four_tdm$dimnames$Terms[grep("^a case of", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^would mean the", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^make me the", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^struggling but the", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^struggling but the ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^date at the ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^be on my", four_tdm$dimnames$Terms)]
str(four_tdm)
four_tdm$dimnames$Terms[grep("^in quite some", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^with his little ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^faith during the ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("you must be ", four_tdm$dimnames$Terms)]
source('~/.active-rstudio-document', echo=TRUE)
four_tdm$dimnames$Terms[grep("^a case of ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^would mean the ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^make me the ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^struggling but the ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^date at the ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^be on my ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^in quite some ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^with his little ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^faith during the ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^you must be ", four_tdm$dimnames$Terms)]
source('~/.active-rstudio-document', echo=TRUE)
four_tdm$dimnames$Terms[grep("a case of ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("would mean the ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("struggling but the ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^but the ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^but the offense ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^but the crowd ", four_tdm$dimnames$Terms)]
four_tdm$i[grep("^but the crowd ", four_tdm$dimnames$Terms)]
four_tdm$j[grep("^but the crowd ", four_tdm$dimnames$Terms)]
four_tdm$v[grep("^but the crowd ", four_tdm$dimnames$Terms)]
four_tdm$v[grep("^but the referees ", four_tdm$dimnames$Terms)]
four_tdm$v[grep("^but the defense ", four_tdm$dimnames$Terms)]
four_tdm$v[grep("^but the players ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^date at the ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^with his little ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^his little ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^faith during the ", four_tdm$dimnames$Terms)]
four_tdm$dimnames$Terms[grep("^during the ", four_tdm$dimnames$Terms)]
four_tdm$v[grep("^his little ", four_tdm$dimnames$Terms)]
four_tdm$v[grep("^his little fingers ", four_tdm$dimnames$Terms)]
insepct(td["his little fingers ", dimnames(tdm)$Docs])
inspect(td["his little fingers ", dimnames(tdm)$Docs])
inspect(four_tdm["his little fingers ", dimnames(four_tdm)$Docs])
inspect(four_tdm["during the year i", dimnames(four_tdm)$Docs])
rowSums(inspect(four_tdm["during the year i", dimnames(four_tdm)$Docs]))
rowSums(inspect(four_tdm["be on my way", dimnames(four_tdm)$Docs]))
four_tdm$dimnames[grep("^during the ", four_tdm$dimnames$Terms)]
four_tdm$v[grep("^during the ", four_tdm$dimnames$Terms)]
sum(four_tdm$v[grep("be on my way", four_tdm$dimnames$Terms)])
sum(four_tdm$v[grep("with his little eyes", four_tdm$dimnames$Terms)])
sum(four_tdm$v[grep("with his little fingers", four_tdm$dimnames$Terms)])
sum(four_tdm$v[grep("eyes with his fingers", four_tdm$dimnames$Terms)])
sum(four_tdm$v[grep("eyes with his ears", four_tdm$dimnames$Terms)])
sum(four_tdm$v[grep("eyes with his toes", four_tdm$dimnames$Terms)])
sum(four_tdm$v[grep("you must be callous", four_tdm$dimnames$Terms)])
sum(four_tdm$v[grep("you must be insensitive", four_tdm$dimnames$Terms)])
sum(four_tdm$v[grep("you must be insane", four_tdm$dimnames$Terms)])
sum(four_tdm$v[grep("you must be asleep", four_tdm$dimnames$Terms)])
sum(four_tdm$v[grep("must be asleep ", four_tdm$dimnames$Terms)])
sum(four_tdm$v[grep("must be insane ", four_tdm$dimnames$Terms)])
sum(four_tdm$v[grep("must be insensitive ", four_tdm$dimnames$Terms)])
sum(four_tdm$v[grep("must be callous ", four_tdm$dimnames$Terms)])
gc()
rm(list=ls())
gc()
gcinfo(TRUE)
gc
gc()
setwd('data_science_capstone/')
dir
ls
dir()
library(data.table)
#read in list of curse words.
curse_words = readLines("swearWords.txt")
curse_words = paste(curse_words, collapse = "\\b|\\b")
curse_words = paste(paste("\\b", curse_words, sep=""), "\\b", sep="")
# 1 gram term document matrix
final_one_tdm < read.table('final/training/one_tdm.csv')
final_one_tdm <- final_one_tdm[order(freq, decreasing=TRUE)]
setkey(final_one_tdm, n0)
dir()
library(data.table)
#read in list of curse words.
curse_words = readLines("final/swearWords.txt")
curse_words = paste(curse_words, collapse = "\\b|\\b")
curse_words = paste(paste("\\b", curse_words, sep=""), "\\b", sep="")
final_one_tdm < read.table('final/training/one_tdm.csv')
final_one_tdm <- final_one_tdm[order(freq, decreasing=TRUE)]
setkey(final_one_tdm, n0)
final_one_tdm <- read.table('final/training/one_tdm.csv')
final_one_tdm <- final_one_tdm[order(freq, decreasing=TRUE)]
setkey(final_one_tdm, n0)
head(final_one_tdm)
final_one_tdm <- read.table('final/training/one_tdm.csv', header=TRUE)
final_one_tdm <- final_one_tdm[order(freq, decreasing=TRUE)]
setkey(final_one_tdm, n0)
head(final_one_tdm)
final_one_tdm <- read.table('final/training/one_tdm.csv', header=TRUE)
final_one_tdm <- final_one_tdm[order(final_one_tdm$freq, decreasing=TRUE)]
setkey(final_one_tdm, n0)
final_one_tdm <- fread('final/training/one_tdm.csv', header=TRUE)
final_one_tdm
final_one_tdm <- final_one_tdm[order(freq, decreasing=TRUE)]
setkey(final_one_tdm, n0)
library(data.table)
#read in list of curse words.
curse_words = readLines("final/swearWords.txt")
curse_words = paste(curse_words, collapse = "\\b|\\b")
curse_words = paste(paste("\\b", curse_words, sep=""), "\\b", sep="")
# 1 gram term document matrix
final_one_tdm <- fread('final/training/one_tdm.csv', header=TRUE)
final_one_tdm <- final_one_tdm[order(freq, decreasing=TRUE)]
setkey(final_one_tdm, n0)
# 2 gram term document matrix
final_two_tdm <- fread('final/training/two_tdm.csv', header=TRUE)
final_two_tdm <- final_two_tdm[final_two_tdm$n0 != ' ' & final_two_tdm$n1 != ' ']
setkey(final_two_tdm, n0, n1)
# 3 gram term document matrix
final_three_tdm <- fread('final/training/three_tdm.csv', header=TRUE)
final_three_tdm <- final_three_tdm[final_three_tdm$n0 != ' ' & final_three_tdm$n1 != ' ' & final_three_tdm$n2 != ' ']
setkey(final_three_tdm, n0, n1, n2)
final_three_tdm$n0
final_three_tdm[final_three_tdm$n0 == "STOP"]
final_three_tdm[final_three_tdm$n2 == "START"]
final_three_tdm[final_three_tdm$n2 == "START" & final_three_tdm$n1 == "the"]
final_three_tdm[final_three_tdm$n2 == "START" & final_three_tdm$n1 == "the" & final_three_tdm$freq > 100]
final_three_tdm[final_three_tdm$n2 == "START" & final_three_tdm$n1 == "the" & final_three_tdm$freq > 200]
final_three_tdm[final_three_tdm$n2 == "START" & final_three_tdm$n1 == "the" & final_three_tdm$freq > 400]
final_three_tdm[final_three_tdm$n2 == "START" & final_three_tdm$n1 == "the" & final_three_tdm$freq > 800]
final_three_tdm[final_three_tdm$n2 == "START" & final_three_tdm$n1 == "the" & final_three_tdm$freq > 1600]
final_three_tdm[final_three_tdm$n2 == "START" & final_three_tdm$n1 == "you" & final_three_tdm$freq > 1600]
